<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>&#39;搜狗全网新闻数据处理&#39;</title>
      <link href="/%E6%90%9C%E7%8B%97%E5%85%A8%E7%BD%91%E6%96%B0%E9%97%BB%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/"/>
      <url>/%E6%90%9C%E7%8B%97%E5%85%A8%E7%BD%91%E6%96%B0%E9%97%BB%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> jieba.analyse</span><br></pre></td></tr></table></figure><h1 id="搜狗全网新闻数据-SogouCA-预处理"><a href="#搜狗全网新闻数据-SogouCA-预处理" class="headerlink" title="搜狗全网新闻数据(SogouCA)预处理"></a>搜狗全网新闻数据(SogouCA)预处理</h1><ul><li>逐个读取这些txt文件内容</li><li>利用正则表达式匹配出URL(新闻类别)和Content(新闻内容)</li><li>主要文件</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">STRLEN = <span class="number">30</span>  <span class="comment">#字符数小于30的content将不被保存</span></span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_extract</span><span class="params">(url)</span>:</span></span><br><span class="line">    <span class="string">'''匹配获取url中的label类别'''</span></span><br><span class="line">    pattern_label = re.compile(<span class="string">r'http://(.*)'</span>, re.S)</span><br><span class="line">    url_body = pattern_label.findall(url.strip())[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">return</span> url_body.split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">file_match</span><span class="params">(file)</span>:</span></span><br><span class="line">    <span class="string">'''匹配获取txt文件中的url和Conent'''</span></span><br><span class="line">    pattern_url = re.compile(<span class="string">r'&lt;url&gt;(.*?)&lt;/url&gt;'</span>, re.S)</span><br><span class="line">    pattern_content = re.compile(<span class="string">r'&lt;content&gt;(.*?)&lt;/content&gt;'</span>, re.S)</span><br><span class="line">    urls = pattern_url.findall(file)</span><br><span class="line">    contents = pattern_content.findall(file)</span><br><span class="line">    <span class="keyword">return</span> urls, contents</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sample_process</span><span class="params">(origin_dir, sample_dir=<span class="string">'./data/sample'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''对原始数据进行预处理，提取原始数据的url和content标签内容，提取url中包含的label，把content写入以label为文件名的txt文件中'''</span></span><br><span class="line">    <span class="comment">#查看file_dir下的文件与目录</span></span><br><span class="line">    <span class="keyword">for</span> _, _, files <span class="keyword">in</span> os.walk(origin_dir):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(sample_dir):</span><br><span class="line">            os.makedirs(sample_dir)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            origin_file = os.path.join(origin_dir, file)</span><br><span class="line">            <span class="comment">#遍历原始数据文件</span></span><br><span class="line">            <span class="keyword">with</span> open(origin_file, <span class="string">'r'</span>, encoding=<span class="string">'ANSI'</span>, errors=<span class="string">'ignore'</span>) <span class="keyword">as</span> f:</span><br><span class="line">                file_content = f.read()</span><br><span class="line">                urls, contents = file_match(file_content)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(urls)):</span><br><span class="line">                    <span class="keyword">if</span> len(contents[i]) &lt; STRLEN:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    label = label_extract(urls[i])</span><br><span class="line">                    sample_file = os.path.join(sample_dir, label) + <span class="string">'.txt'</span></span><br><span class="line">                    <span class="keyword">with</span> open(sample_file, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> sf:</span><br><span class="line">                        sf.write(contents[i] + <span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">'数据已预处理'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sample_process(<span class="string">'./data/SogouCS.reduced'</span>)</span><br></pre></td></tr></table></figure><pre><code>数据已预处理</code></pre><h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><ul><li>在数据预处理基础上进一步对数据格式进行清洗</li><li>包括删除特殊字符，去除标点和中文分词等工作</li><li>需要自定义函数，并使用相关工具如jieba等</li></ul><h2 id="提取文本中汉字数据"><a href="#提取文本中汉字数据" class="headerlink" title="提取文本中汉字数据"></a>提取文本中汉字数据</h2><ul><li>可以直接通过提取文章中的汉字方式过滤数字和特殊符号</li><li>去除中文停用词（采用哈工大中文停用词库）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取停用词列表</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/哈工大停用词表.txt'</span>, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    stopwords = f.readlines()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">isHans</span><span class="params">(strs)</span>:</span></span><br><span class="line">    <span class="string">'''判断词语是否为汉字'''</span></span><br><span class="line">    pat = <span class="string">r'[\u4e00-\u9fa5]+'</span></span><br><span class="line">    <span class="keyword">return</span> bool(re.match(pat, strs))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">check</span><span class="params">(word)</span>:</span></span><br><span class="line">    <span class="string">'''判断分词的基本特征'''</span></span><br><span class="line">    word = word.strip().replace(<span class="string">' '</span>, <span class="string">''</span>)</span><br><span class="line">    <span class="keyword">if</span> isHans(word):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocessing</span><span class="params">(sens)</span>:</span></span><br><span class="line">    <span class="string">'''综合处理'''</span></span><br><span class="line">    res = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sens:</span><br><span class="line">        <span class="keyword">if</span> check(word):</span><br><span class="line">            res.append(word)</span><br><span class="line">    <span class="keyword">return</span> res</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_files</span><span class="params">(file_path, new_path)</span>:</span></span><br><span class="line">    <span class="string">'''读取处理文件数据'''</span></span><br><span class="line">    <span class="comment"># 创建new_path存储处理好的文件</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(new_path):</span><br><span class="line">        os.makedirs(new_path)</span><br><span class="line">    <span class="comment"># 进入预处理后的主文件夹sample</span></span><br><span class="line">    <span class="keyword">for</span> _, _, files <span class="keyword">in</span> os.walk(file_path):</span><br><span class="line">        print(<span class="string">'sample的lables：'</span>)</span><br><span class="line">        print(files)</span><br><span class="line">        print(<span class="string">'每一个labels下有效数据的数量：'</span>)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            d_p_file = os.path.join(file_path, file)</span><br><span class="line">            n_d_file = os.path.join(new_path, file)</span><br><span class="line">            <span class="comment">#打开分类文件</span></span><br><span class="line">            <span class="keyword">with</span> open(d_p_file, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f1:</span><br><span class="line">                docs = f1.readlines()</span><br><span class="line">                <span class="comment"># 创建一个空列表收集处理后的句子</span></span><br><span class="line">                res_docs = []</span><br><span class="line">                <span class="comment"># 读取每一行的数据</span></span><br><span class="line">                <span class="keyword">for</span> docs <span class="keyword">in</span> docs:</span><br><span class="line">                    <span class="comment">#对每行数据进行分词</span></span><br><span class="line">                    sens = jieba.cut(docs)</span><br><span class="line">                    <span class="comment"># 提取文本中的汉语数据</span></span><br><span class="line">                    res = preprocessing(sens)</span><br><span class="line">                    result = <span class="string">''</span>.join(res)</span><br><span class="line">                    result += <span class="string">'\n'</span></span><br><span class="line">                    res_docs.append(result)</span><br><span class="line">                print(len(res_docs))</span><br><span class="line">                <span class="comment"># 将处理好后的res_docs里的句子写入新文件</span></span><br><span class="line">                <span class="keyword">with</span> open(n_d_file, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f2:</span><br><span class="line">                    <span class="keyword">for</span> docum <span class="keyword">in</span> res_docs:</span><br><span class="line">                        f2.write(docum)</span><br><span class="line">                    print(<span class="string">'文件已写入'</span>)</span><br><span class="line">    print(<span class="string">'文章处理完毕'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据格式化</span></span><br><span class="line">sample_path = <span class="string">'./data/sample'</span></span><br><span class="line">format_path = <span class="string">'./data/format'</span></span><br><span class="line">handle_files(sample_path, format_path)</span><br></pre></td></tr></table></figure><pre><code>sample的lables：[&apos;2008.txt&apos;, &apos;auto.txt&apos;, &apos;business.txt&apos;, &apos;career.txt&apos;, &apos;cul.txt&apos;, &apos;health.txt&apos;, &apos;house.txt&apos;, &apos;it.txt&apos;, &apos;learning.txt&apos;, &apos;mil.txt&apos;, &apos;news.txt&apos;, &apos;sports.txt&apos;, &apos;travel.txt&apos;, &apos;women.txt&apos;, &apos;yule.txt&apos;]每一个labels下有效数据的数量：223文件已写入62文件已写入471文件已写入2文件已写入29文件已写入41文件已写入524文件已写入75文件已写入61文件已写入25文件已写入630文件已写入623文件已写入69文件已写入113文件已写入237文件已写入文章处理完毕</code></pre><h1 id="获取训练集及测试集的分类数据"><a href="#获取训练集及测试集的分类数据" class="headerlink" title="获取训练集及测试集的分类数据"></a>获取训练集及测试集的分类数据</h1><ul><li>为使项目结构更加清晰，可以建立多层文件夹路径</li><li>利用os.walk和os.makedirs等方法</li><li>对格式化数据均匀分成训练集和测试集</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">allocate_data</span><span class="params">(file_path, train_path, test_path)</span>:</span></span><br><span class="line">    <span class="string">'''将格式化后的数据均匀分布到测试集和训练集中'''</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(train_path):</span><br><span class="line">        os.makedirs(train_path)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(test_path):</span><br><span class="line">        os.makedirs(test_path)</span><br><span class="line">    <span class="comment">#获取格式化的数据文件</span></span><br><span class="line">    <span class="keyword">for</span> _, _, files <span class="keyword">in</span> os.walk(file_path):</span><br><span class="line">        <span class="comment"># 遍历数据文件</span></span><br><span class="line">        print(<span class="string">'已处理的文件：'</span>)</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            print(file)</span><br><span class="line">            d_f_file = os.path.join(file_path, file)</span><br><span class="line">            d_train_file = os.path.join(train_path, file)</span><br><span class="line">            d_test_file = os.path.join(test_path, file)</span><br><span class="line">            <span class="keyword">with</span> open(d_f_file, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f1:</span><br><span class="line">                docs = f1.readlines()</span><br><span class="line">                <span class="comment"># 读取文件中的每一行，偶数行做测试数据，奇数行做训练数据</span></span><br><span class="line">                <span class="keyword">for</span> index, doc <span class="keyword">in</span> enumerate(docs):</span><br><span class="line">                    <span class="comment"># index为偶数</span></span><br><span class="line">                    <span class="keyword">if</span> (index &amp; <span class="number">1</span>) == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">with</span> open(d_test_file, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f2:</span><br><span class="line">                            f2.write(doc)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        <span class="keyword">with</span> open(d_train_file, <span class="string">'a+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f3:</span><br><span class="line">                            f3.write(doc)</span><br><span class="line">    print(<span class="string">'训练集和测试集已完成'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">format_path = <span class="string">'./data/format'</span></span><br><span class="line">train_path = <span class="string">'./data/train'</span></span><br><span class="line">test_path = <span class="string">'./data/test'</span></span><br><span class="line">allocate_data(format_path, train_path, test_path)</span><br></pre></td></tr></table></figure><pre><code>已处理的文件：2008.txtauto.txtbusiness.txtcareer.txtcul.txthealth.txthouse.txtit.txtlearning.txtmil.txtnews.txtsports.txttravel.txtwomen.txtyule.txt训练集和测试集已完成</code></pre><h1 id="列表化训练集和测试集数据"><a href="#列表化训练集和测试集数据" class="headerlink" title="列表化训练集和测试集数据"></a>列表化训练集和测试集数据</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_cont2list</span><span class="params">(data_path)</span>:</span></span><br><span class="line">    <span class="string">'''该函数用来测试数据矩阵,返回数据集的标签列表和内容列表'''</span></span><br><span class="line">    <span class="comment"># 定义文件内容和标签列表</span></span><br><span class="line">    file_cont_list, file_label_list = [], []</span><br><span class="line">    <span class="keyword">for</span> _, _, files <span class="keyword">in</span> os.walk(data_path):</span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> files:</span><br><span class="line">            file_name = os.path.join(data_path, file)</span><br><span class="line">            <span class="comment">#读取文件</span></span><br><span class="line">            <span class="keyword">with</span> open(file_name, <span class="string">'r'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f1:</span><br><span class="line">                <span class="comment">#将文件内的每行数据作为元素存入列表</span></span><br><span class="line">                file_c_list = [k.strip() <span class="keyword">for</span> k <span class="keyword">in</span> f1.readlines()]</span><br><span class="line">            file_cont_list.extend(file_c_list)</span><br><span class="line">            file_label_list.append(file)</span><br><span class="line">    <span class="keyword">return</span> file_label_list, file_cont_list</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取训练数据集的标签列表和内容列表</span></span><br><span class="line">train_path = <span class="string">'./data/train'</span></span><br><span class="line">train_label_list, train_cont_list = label_cont2list(train_path)</span><br><span class="line"></span><br><span class="line"><span class="comment">#获取测试数据集的标签列表和内容列表</span></span><br><span class="line">test_path = <span class="string">'./data/test'</span></span><br><span class="line">test_label_list, test_cont_list = label_cont2list(test_path)</span><br></pre></td></tr></table></figure><h1 id="生成词汇表"><a href="#生成词汇表" class="headerlink" title="生成词汇表"></a>生成词汇表</h1><ul><li>收集训练集文本内容列表词作为词汇表</li><li>使用TF-IDF提取关键词</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getVocabList</span><span class="params">(content_list, vocab_size)</span>:</span></span><br><span class="line">    <span class="string">'''定义获取词汇表的函数'''</span></span><br><span class="line">    <span class="comment">#     print(type(content_list[0]))</span></span><br><span class="line">    str_allContent = <span class="string">''</span>.join(content_list)</span><br><span class="line">    vocab_list = jieba.analyse.extract_tags(str_allContent, topK=vocab_size)</span><br><span class="line">    <span class="comment">#     vocab_list = [k[0] for k in counter.most_common(vocab_size)]</span></span><br><span class="line">    <span class="keyword">return</span> vocab_list</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateVocabList</span><span class="params">(content_list, vocab_size)</span>:</span></span><br><span class="line">    vocab_list = getVocabList(content_list, vocab_size)</span><br><span class="line">    print(vocab_list[:<span class="number">10</span>])</span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'./data/vocab_list.txt'</span>, <span class="string">'w+'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> vocab <span class="keyword">in</span> vocab_list:</span><br><span class="line">            f.write(vocab + <span class="string">'\n'</span>)</span><br><span class="line">    print(<span class="string">'词汇表创建完成'</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">generateVocabList(train_cont_list, <span class="number">500</span>)</span><br></pre></td></tr></table></figure><pre><code>[&apos;责任编辑&apos;, &apos;搜狐&apos;, &apos;公司&apos;, &apos;奥运&apos;, &apos;比赛&apos;, &apos;中国&apos;, &apos;我们&apos;, &apos;地震&apos;, &apos;灾区&apos;, &apos;市场&apos;]词汇表创建完成</code></pre><h1 id="词汇表向量化"><a href="#词汇表向量化" class="headerlink" title="词汇表向量化"></a>词汇表向量化</h1><ul><li>利用词汇表文件将数据进行词向量化</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#读取词汇表文件数据，存入列表中</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./data/vocab_list.txt'</span>, encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    vocablist = [k.strip() <span class="keyword">for</span> k <span class="keyword">in</span> f.readlines()]</span><br><span class="line"></span><br><span class="line"><span class="comment">#将词汇表中的数据转化为键为数据，值为序号的字词汇点</span></span><br><span class="line">word2id_dict = dict([(b, a) <span class="keyword">for</span> a, b <span class="keyword">in</span> enumerate(vocablist)])</span><br><span class="line">print(word2id_dict)</span><br></pre></td></tr></table></figure><pre><code>{&apos;责任编辑&apos;: 0, &apos;搜狐&apos;: 1, &apos;公司&apos;: 2, &apos;奥运&apos;: 3, &apos;比赛&apos;: 4, &apos;中国&apos;: 5, &apos;我们&apos;: 6, &apos;地震&apos;: 7, &apos;灾区&apos;: 8, &apos;市场&apos;: 9, &apos;记者&apos;: 10, &apos;北京&apos;: 11, &apos;一个&apos;: 12, &apos;进行&apos;: 13, &apos;企业&apos;: 14, &apos;自己&apos;: 15, &apos;没有&apos;: 16, &apos;工作&apos;: 17, &apos;目前&apos;: 18, &apos;时间&apos;: 19, &apos;年月日&apos;: 20, &apos;他们&apos;: 21, &apos;可以&apos;: 22, &apos;问题&apos;: 23, &apos;发展&apos;: 24, &apos;传递&apos;: 25, &apos;已经&apos;: 26, &apos;火炬手&apos;: 27, &apos;项目&apos;: 28, &apos;合作&apos;: 29, &apos;表示&apos;: 30, &apos;来源&apos;: 31, &apos;欧洲杯&apos;: 32, &apos;奥运会&apos;: 33, &apos;抗震救灾&apos;: 34, &apos;情况&apos;: 35, &apos;考生&apos;: 36, &apos;可能&apos;: 37, &apos;媒体&apos;: 38, &apos;这个&apos;: 39, &apos;通过&apos;: 40, &apos;认为&apos;: 41, &apos;国际&apos;: 42, &apos;成为&apos;: 43, &apos;价格&apos;: 44, &apos;国家&apos;: 45, &apos;一些&apos;: 46, &apos;万元&apos;: 47, &apos;就是&apos;: 48, &apos;火炬&apos;: 49, &apos;希望&apos;: 50, &apos;开始&apos;: 51, &apos;美国&apos;: 52, &apos;点击&apos;: 53, &apos;证券&apos;: 54, &apos;孩子&apos;: 55, &apos;如果&apos;: 56, &apos;风险&apos;: 57, &apos;院校&apos;: 58, &apos;现在&apos;: 59, &apos;但是&apos;: 60, &apos;投资&apos;: 61, &apos;志愿&apos;: 62, &apos;湖人&apos;: 63, &apos;房地产&apos;: 64, &apos;资讯&apos;: 65, &apos;产品&apos;: 66, &apos;经济&apos;: 67, &apos;球员&apos;: 68, &apos;政府&apos;: 69, &apos;小区&apos;: 70, &apos;建设&apos;: 71, &apos;今年&apos;: 72, &apos;影响&apos;: 73, &apos;由于&apos;: 74, &apos;同时&apos;: 75, &apos;出现&apos;: 76, &apos;球队&apos;: 77, &apos;户型&apos;: 78, &apos;非常&apos;: 79, &apos;服务&apos;: 80, &apos;主要&apos;: 81, &apos;对于&apos;: 82, &apos;来说&apos;: 83, &apos;这些&apos;: 84, &apos;体育讯&apos;: 85, &apos;因为&apos;: 86, &apos;女排&apos;: 87, &apos;官网&apos;: 88, &apos;圣火&apos;: 89, &apos;这样&apos;: 90, &apos;活动&apos;: 91, &apos;提供&apos;: 92, &apos;相关&apos;: 93, &apos;微软&apos;: 94, &apos;安全&apos;: 95, &apos;社会&apos;: 96, &apos;内容&apos;: 97, &apos;体育&apos;: 98, &apos;作为&apos;: 99, &apos;销售&apos;: 100, &apos;汶川&apos;: 101, &apos;资金&apos;: 102, &apos;很多&apos;: 103, &apos;城市&apos;: 104, &apos;需要&apos;: 105, &apos;进入&apos;: 106, &apos;机构&apos;: 107, &apos;规定&apos;: 108, &apos;学校&apos;: 109, &apos;过客&apos;: 110, &apos;时候&apos;: 111, &apos;建议&apos;: 112, &apos;文化&apos;: 113, &apos;投资者&apos;: 114, &apos;还是&apos;: 115, &apos;以及&apos;: 116, &apos;管理&apos;: 117, &apos;社区&apos;: 118, &apos;旅游&apos;: 119, &apos;重建&apos;: 120, &apos;了解&apos;: 121, &apos;上海&apos;: 122, &apos;有关&apos;: 123, &apos;参加&apos;: 124, &apos;行业&apos;: 125, &apos;集团&apos;: 126, &apos;要求&apos;: 127, &apos;其中&apos;: 128, &apos;信息&apos;: 129, &apos;增长&apos;: 130, &apos;发生&apos;: 131, &apos;评论&apos;: 132, &apos;代表&apos;: 133, &apos;之后&apos;: 134, &apos;亿元&apos;: 135, &apos;开发&apos;: 136, &apos;不是&apos;: 137, &apos;部门&apos;: 138, &apos;专业&apos;: 139, &apos;人员&apos;: 140, &apos;开发商&apos;: 141, &apos;大家&apos;: 142, &apos;国内&apos;: 143, &apos;基金&apos;: 144, &apos;录取&apos;: 145, &apos;高考&apos;: 146, &apos;一定&apos;: 147, &apos;香港&apos;: 148, &apos;不能&apos;: 149, &apos;最后&apos;: 150, &apos;凯尔特人&apos;: 151, &apos;使用&apos;: 152, &apos;重要&apos;: 153, &apos;介绍&apos;: 154, &apos;四川&apos;: 155, &apos;方式&apos;: 156, &apos;我国&apos;: 157, &apos;本频道&apos;: 158, &apos;全国&apos;: 159, &apos;转引&apos;: 160, &apos;有限公司&apos;: 161, &apos;世界&apos;: 162, &apos;因此&apos;: 163, &apos;对此&apos;: 164, &apos;编号&apos;: 165, &apos;训练&apos;: 166, &apos;位置&apos;: 167, &apos;其他&apos;: 168, &apos;正在&apos;: 169, &apos;群众&apos;: 170, &apos;均价&apos;: 171, &apos;关注&apos;: 172, &apos;现场&apos;: 173, &apos;方面&apos;: 174, &apos;电话&apos;: 175, &apos;第一&apos;: 176, &apos;稳定&apos;: 177, &apos;达到&apos;: 178, &apos;业主&apos;: 179, &apos;应该&apos;: 180, &apos;学生&apos;: 181, &apos;以上&apos;: 182, &apos;看到&apos;: 183, &apos;根据&apos;: 184, &apos;生活&apos;: 185, &apos;虽然&apos;: 186, &apos;组织&apos;: 187, &apos;特别&apos;: 188, &apos;教育&apos;: 189, &apos;一直&apos;: 190, &apos;自身&apos;: 191, &apos;计划&apos;: 192, &apos;还有&apos;: 193, &apos;或者&apos;: 194, &apos;技术&apos;: 195, &apos;网友&apos;: 196, &apos;表现&apos;: 197, &apos;继续&apos;: 198, &apos;行政&apos;: 199, &apos;提高&apos;: 200, &apos;报道&apos;: 201, &apos;不过&apos;: 202, &apos;超过&apos;: 203, &apos;科比&apos;: 204, &apos;中心&apos;: 205, &apos;部分&apos;: 206, &apos;所以&apos;: 207, &apos;比较&apos;: 208, &apos;汽车&apos;: 209, &apos;获得&apos;: 210, &apos;灾后&apos;: 211, &apos;举行&apos;: 212, &apos;两句&apos;: 213, &apos;不会&apos;: 214, &apos;招生&apos;: 215, &apos;接受&apos;: 216, &apos;得到&apos;: 217, &apos;北京奥运&apos;: 218, &apos;生产&apos;: 219, &apos;受灾&apos;: 220, &apos;包括&apos;: 221, &apos;政策&apos;: 222, &apos;设计&apos;: 223, &apos;观点&apos;: 224, &apos;帮助&apos;: 225, &apos;实现&apos;: 226, &apos;能够&apos;: 227, &apos;个人&apos;: 228, &apos;彩票&apos;: 229, &apos;主持人&apos;: 230, &apos;建筑面积&apos;: 231, &apos;房子&apos;: 232, &apos;银行&apos;: 233, &apos;选择&apos;: 234, &apos;调整&apos;: 235, &apos;广告&apos;: 236, &apos;盖茨&apos;: 237, &apos;判断&apos;: 238, &apos;这次&apos;: 239, &apos;意大利&apos;: 240, &apos;入市&apos;: 241, &apos;一起&apos;: 242, &apos;一次&apos;: 243, &apos;地铁&apos;: 244, &apos;消费者&apos;: 245, &apos;据此&apos;: 246, &apos;加强&apos;: 247, &apos;最大&apos;: 248, &apos;为了&apos;: 249, &apos;台湾&apos;: 250, &apos;全球&apos;: 251, &apos;别墅&apos;: 252, &apos;原因&apos;: 253, &apos;这种&apos;: 254, &apos;发行&apos;: 255, &apos;增加&apos;: 256, &apos;而且&apos;: 257, &apos;消费&apos;: 258, &apos;去年&apos;: 259, &apos;什么&apos;: 260, &apos;声明&apos;: 261, &apos;造成&apos;: 262, &apos;分析&apos;: 263, &apos;面积&apos;: 264, &apos;股份&apos;: 265, &apos;股东&apos;: 266, &apos;标题&apos;: 267, &apos;发现&apos;: 268, &apos;立场&apos;: 269, &apos;花园&apos;: 270, &apos;所有&apos;: 271, &apos;环境&apos;: 272, &apos;成都&apos;: 273, &apos;品牌&apos;: 274, &apos;上涨&apos;: 275, &apos;未来&apos;: 276, &apos;能力&apos;: 277, &apos;资本&apos;: 278, &apos;昨天&apos;: 279, &apos;资产&apos;: 280, &apos;消息&apos;: 281, &apos;单位&apos;: 282, &apos;日电&apos;: 283, &apos;房屋&apos;: 284, &apos;精彩图片&apos;: 285, &apos;基本&apos;: 286, &apos;报告&apos;: 287, &apos;建筑&apos;: 288, &apos;拥有&apos;: 289, &apos;过程&apos;: 290, &apos;美元&apos;: 291, &apos;人民币&apos;: 292, &apos;赛季&apos;: 293, &apos;中国队&apos;: 294, &apos;支持&apos;: 295, &apos;装修&apos;: 296, &apos;新闻&apos;: 297, &apos;下跌&apos;: 298, &apos;楼层&apos;: 299, &apos;今天&apos;: 300, &apos;不同&apos;: 301, &apos;分钟&apos;: 302, &apos;室厅&apos;: 303, &apos;本科&apos;: 304, &apos;完成&apos;: 305, &apos;小时&apos;: 306, &apos;分别&apos;: 307, &apos;采访&apos;: 308, &apos;系统&apos;: 309, &apos;规划&apos;: 310, &apos;合同&apos;: 311, &apos;成绩&apos;: 312, &apos;平方米&apos;: 313, &apos;一位&apos;: 314, &apos;带来&apos;: 315, &apos;志愿者&apos;: 316, &apos;必须&apos;: 317, &apos;谨慎&apos;: 318, &apos;业务&apos;: 319, &apos;日本&apos;: 320, &apos;制度&apos;: 321, &apos;本次&apos;: 322, &apos;优势&apos;: 323, &apos;负责人&apos;: 324, &apos;人民&apos;: 325, &apos;昨日&apos;: 326, &apos;人士&apos;: 327, &apos;恢复&apos;: 328, &apos;历史&apos;: 329, &apos;成功&apos;: 330, &apos;总决赛&apos;: 331, &apos;此次&apos;: 332, &apos;准备&apos;: 333, &apos;冠军&apos;: 334, &apos;他人&apos;: 335, &apos;机会&apos;: 336, &apos;正式&apos;: 337, &apos;跳转&apos;: 338, &apos;比例&apos;: 339, &apos;决赛&apos;: 340, &apos;不断&apos;: 341, &apos;唯一&apos;: 342, &apos;地产&apos;: 343, &apos;下午&apos;: 344, &apos;担责任&apos;: 345, &apos;甚至&apos;: 346, &apos;韩国&apos;: 347, &apos;竞争&apos;: 348, &apos;知道&apos;: 349, &apos;最终&apos;: 350, &apos;控制&apos;: 351, &apos;两个&apos;: 352, &apos;成本&apos;: 353, &apos;专家&apos;: 354, &apos;状态&apos;: 355, &apos;结果&apos;: 356, &apos;房价&apos;: 357, &apos;对手&apos;: 358, &apos;作者&apos;: 359, &apos;直接&apos;: 360, &apos;如何&apos;: 361, &apos;赈灾&apos;: 362, &apos;进一步&apos;: 363, &apos;购买&apos;: 364, &apos;网站&apos;: 365, &apos;需求&apos;: 366, &apos;决定&apos;: 367, &apos;结束&apos;: 368, &apos;当地&apos;: 369, &apos;至页&apos;: 370, &apos;考虑&apos;: 371, &apos;取得&apos;: 372, &apos;新华网&apos;: 373, &apos;联系&apos;: 374, &apos;统一&apos;: 375, &apos;下降&apos;: 376, &apos;当时&apos;: 377, &apos;任何&apos;: 378, &apos;海航&apos;: 379, &apos;随着&apos;: 380, &apos;客户&apos;: 381, &apos;层楼&apos;: 382, &apos;人们&apos;: 383, &apos;第届&apos;: 384, &apos;标准&apos;: 385, &apos;按照&apos;: 386, &apos;经过&apos;: 387, &apos;散布&apos;: 388, &apos;只有&apos;: 389, &apos;一种&apos;: 390, &apos;持续&apos;: 391, &apos;行为&apos;: 392, &apos;年度&apos;: 393, &apos;经营&apos;: 394, &apos;实施&apos;: 395, &apos;食品&apos;: 396, &apos;住房&apos;: 397, &apos;之前&apos;: 398, &apos;网络&apos;: 399, &apos;公开&apos;: 400, &apos;其实&apos;: 401, &apos;地区&apos;: 402, &apos;关系&apos;: 403, &apos;手机&apos;: 404, &apos;全面&apos;: 405, &apos;参与&apos;: 406, &apos;交通&apos;: 407, &apos;提出&apos;: 408, &apos;家长&apos;: 409, &apos;城区&apos;: 410, &apos;新城&apos;: 411, &apos;广场&apos;: 412, &apos;民族&apos;: 413, &apos;交通状况&apos;: 414, &apos;观众&apos;: 415, &apos;保证&apos;: 416, &apos;预计&apos;: 417, &apos;上市&apos;: 418, &apos;更加&apos;: 419, &apos;导致&apos;: 420, &apos;一场&apos;: 421, &apos;显示&apos;: 422, &apos;图为&apos;: 423, &apos;平米&apos;: 424, &apos;因素&apos;: 425, &apos;娱乐&apos;: 426, &apos;土地&apos;: 427, &apos;开展&apos;: 428, &apos;告诉&apos;: 429, &apos;之间&apos;: 430, &apos;空间&apos;: 431, &apos;规模&apos;: 432, &apos;荷兰队&apos;: 433, &apos;积极&apos;: 434, &apos;高层&apos;: 435, &apos;队员&apos;: 436, &apos;明显&apos;: 437, &apos;新华社&apos;: 438, &apos;不少&apos;: 439, &apos;存在&apos;: 440, &apos;日前&apos;: 441, &apos;四川省&apos;: 442, &apos;水平&apos;: 443, &apos;改革&apos;: 444, &apos;地址&apos;: 445, &apos;类别&apos;: 446, &apos;之一&apos;: 447, &apos;雅虎&apos;: 448, &apos;抢险&apos;: 449, &apos;领导&apos;: 450, &apos;监督&apos;: 451, &apos;受到&apos;: 452, &apos;售价&apos;: 453, &apos;左右&apos;: 454, &apos;战略&apos;: 455, &apos;地方&apos;: 456, &apos;居民&apos;: 457, &apos;配套&apos;: 458, &apos;来自&apos;: 459, &apos;股权&apos;: 460, &apos;股市&apos;: 461, &apos;集美&apos;: 462, &apos;关于&apos;: 463, &apos;市民&apos;: 464, &apos;压力&apos;: 465, &apos;会议&apos;: 466, &apos;原则&apos;: 467, &apos;双方&apos;: 468, &apos;说明&apos;: 469, &apos;支付&apos;: 470, &apos;有效期&apos;: 471, &apos;原药&apos;: 472, &apos;全文&apos;: 473, &apos;具有&apos;: 474, &apos;喜欢&apos;: 475, &apos;足球&apos;: 476, &apos;完善&apos;: 477, &apos;上午&apos;: 478, &apos;精装&apos;: 479, &apos;一名&apos;: 480, &apos;利率&apos;: 481, &apos;世界杯&apos;: 482, &apos;全部&apos;: 483, &apos;图片&apos;: 484, &apos;解决&apos;: 485, &apos;无法&apos;: 486, &apos;以来&apos;: 487, &apos;年代&apos;: 488, &apos;坚持&apos;: 489, &apos;这是&apos;: 490, &apos;另外&apos;: 491, &apos;印度&apos;: 492, &apos;上市公司&apos;: 493, &apos;尽管&apos;: 494, &apos;不仅&apos;: 495, &apos;老师&apos;: 496, &apos;填报&apos;: 497, &apos;此外&apos;: 498, &apos;一页&apos;: 499}</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">list2vector</span><span class="params">(word2id_dict, train_cont_list)</span>:</span></span><br><span class="line">    <span class="string">'''定义词向量化函数，将数据转化为词向量'''</span></span><br><span class="line">    <span class="comment">#定义一个词汇字典与文本内容的映射关系</span></span><br><span class="line">    cont2id_list = <span class="keyword">lambda</span> content: [</span><br><span class="line">        word2id_dict[word] <span class="keyword">for</span> word <span class="keyword">in</span> content <span class="keyword">if</span> word <span class="keyword">in</span> word2id_dict</span><br><span class="line">    ]</span><br><span class="line">    <span class="comment">#将训练集数据映射为词汇字典对应的词向量</span></span><br><span class="line">    train_idlist_list = [cont2id_list(content) <span class="keyword">for</span> content <span class="keyword">in</span> train_cont_list]</span><br><span class="line">    <span class="keyword">return</span> train_idlist_list</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#获取训练样本分词矩阵</span></span><br><span class="line">train_content_list  = []</span><br><span class="line"><span class="keyword">for</span> train_cont <span class="keyword">in</span> train_cont_list:</span><br><span class="line">    cont_list = list(jieba.cut(train_cont))</span><br><span class="line">    train_content_list.append(cont_list)</span><br><span class="line">    </span><br><span class="line">print(train_content_list[<span class="number">0</span>])</span><br></pre></td></tr></table></figure><pre><code>[&apos;来源&apos;, &apos;第届&apos;, &apos;奥林匹克运动会&apos;, &apos;官方网站&apos;, &apos;作者&apos;, &apos;摄影&apos;, &apos;缪礼&apos;, &apos;东末&apos;, &apos;棒&apos;, &apos;火炬手&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;末棒&apos;, &apos;火炬手&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;末棒&apos;, &apos;火炬手&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;末棒&apos;, &apos;火炬手&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;末棒&apos;, &apos;火炬手&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;奥运&apos;, &apos;官网&apos;, &apos;月&apos;, &apos;日&apos;, &apos;讯&apos;, &apos;北京&apos;, &apos;时间&apos;, &apos;今日&apos;, &apos;点分&apos;, &apos;奥运&apos;, &apos;圣火&apos;, &apos;将&apos;, &apos;首先&apos;, &apos;在&apos;, &apos;广西&apos;, &apos;百色市&apos;, &apos;平果县&apos;, &apos;传递&apos;, &apos;随后&apos;, &apos;转&apos;, &apos;至&apos;, &apos;百色市&apos;, &apos;区&apos;, &apos;继续&apos;, &apos;传递&apos;, &apos;百色&apos;, &apos;是&apos;, &apos;奥运&apos;, &apos;火炬&apos;, &apos;在&apos;, &apos;广西&apos;, &apos;境内&apos;, &apos;传递&apos;, &apos;的&apos;, &apos;最后&apos;, &apos;一站&apos;, &apos;图为&apos;, &apos;末&apos;, &apos;棒&apos;, &apos;火炬手&apos;, &apos;百色市&apos;, &apos;政府&apos;, &apos;副&apos;, &apos;秘书长&apos;, &apos;王军&apos;, &apos;点燃&apos;, &apos;圣&apos;, &apos;火盆&apos;, &apos;奥运&apos;, &apos;官网&apos;, &apos;记者&apos;, &apos;缪礼东&apos;, &apos;发于&apos;, &apos;百色&apos;, &apos;责任编辑&apos;, &apos;郝彦明&apos;]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取训练样本映射的词向量矩阵</span></span><br><span class="line">train_idlist_list = list2vector(word2id_dict,train_content_list)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 毕设 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
            <tag> 数据处理 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>复试规划</title>
      <link href="/%E5%A4%8D%E8%AF%95%E8%A7%84%E5%88%92/"/>
      <url>/%E5%A4%8D%E8%AF%95%E8%A7%84%E5%88%92/</url>
      
        <content type="html"><![CDATA[<h1 id="基础课程"><a href="#基础课程" class="headerlink" title="基础课程"></a>基础课程</h1><ul><li>数据结构与算法</li><li>计算机组成原理</li><li>操作系统</li><li>计算机网络</li><li>汇编语言（开展中）</li><li>微机原理</li><li>数据库系统（开展中）</li><li>离散数学（开展中）</li><li>高等数学</li><li>线性代数</li><li>概率统计</li></ul><h1 id="语言"><a href="#语言" class="headerlink" title="语言"></a>语言</h1><ul><li>python</li><li>java（开展中）</li><li>c</li><li>c++</li><li>html  + css（开展中）</li><li>js（开展中）</li></ul><h1 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h1><h2 id="java"><a href="#java" class="headerlink" title="java"></a>java</h2><ul><li><p>servlet</p></li><li><p>jsp</p></li></ul><h2 id="python"><a href="#python" class="headerlink" title="python"></a>python</h2><ul><li>爬虫（开展中）</li><li>数据分析<ul><li>numpy</li><li>pandas</li><li>Matplotlib</li></ul></li><li>机器学习</li></ul><h2 id="html-css-js"><a href="#html-css-js" class="headerlink" title="html + css + js"></a>html + css + js</h2><ul><li><p>jQuery</p></li><li><p>BootStrap</p></li></ul><h1 id="项目"><a href="#项目" class="headerlink" title="项目"></a>项目</h1><ul><li>爬虫（开展中）<ul><li>中国气象网</li><li>拉钩网</li><li>等等</li></ul></li><li>java<ul><li>java商城项目</li></ul></li><li>操作系统</li><li>前端<ul><li>天猫页面</li></ul></li></ul><h1 id="毕设"><a href="#毕设" class="headerlink" title="毕设"></a>毕设</h1><ul><li>一个基于机器学习的新闻分类系统（开展中）</li></ul><h1 id="刷题机试"><a href="#刷题机试" class="headerlink" title="刷题机试"></a>刷题机试</h1><ul><li>leetcode（开展中）</li></ul>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
